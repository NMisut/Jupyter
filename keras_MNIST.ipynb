{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NMisut/Jupyter/blob/main/keras_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilización de keras para reconocimiento de dígitos manuscritos\n",
        "\n",
        "Datos de partida: utilizaremos el [dataset MNIST, recopilado y compartido por Yann LeCun](http://yann.lecun.com/exdb/mnist/) y que podemos cargar desde la propia librería de Keras.\n",
        "\n"
      ],
      "metadata": {
        "id": "mG2ze-xlKETk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Carga el dataset MNIST de Keras\n",
        "\n",
        "Empezamos por importar el dataset desde la librería Keras y cargarlo con el método load_data()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wptf6-XVOFBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images,test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "id": "N-KYTabPkfPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7651c882-8847-476c-a3ec-89aa1cd74579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Construcción del modelo\n",
        "\n",
        "Importamos los módulos para crear un modelo y para utilizar la arquitectura por capas típica del deep learning. \n",
        "Creamos el objeto network, que será nuestro modelo de la clase Sequential(), que es la que admite varias capas de profundidad.\n",
        "Después, con el método add, iremos apilando capas, en este caso, de la clase Dense, que los las de la arquitectura típica de red neuronal (densely connected). Una primera capa tendrá el atributo de 512 neuronas y función de activación \"relu\" y la siguiente capa tendrá 10 neuronas y función de activación \"softmax\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wl-D_y4sPQuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers \n",
        "\n",
        "network = models.Sequential()\n",
        "network.add(layers.Dense(512, activation = 'relu', input_shape = (28*28,)))\n",
        "network.add(layers.Dense(10, activation = 'softmax'))\n"
      ],
      "metadata": {
        "id": "g8UbsqkHkgTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Fijar los parámetros del entrenamiento\n",
        "\n",
        "Con el método compile(), indicamos el optimizador a utilizar, el tipo de función de coste (la relacionada con el error entre el resultado del modelo y las etiquetas dadas) y la métrica que queremos ir monitorizando durante el entrenamiento, que será 'accuracy' (precisión o casos correctamente clasificados).\n",
        "\n"
      ],
      "metadata": {
        "id": "f6Hx0hkcXqVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network.compile(optimizer = 'rmsprop',\n",
        "                loss = 'categorical_crossentropy',\n",
        "                metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "gkYjGt-HkhyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Preparando las imágenes y las etiquetas\n",
        "\n",
        "Antes de pasar al entrenamiento, necesitamos hacer algunos ajustes. \n",
        "Por un lado, tenemos que cambiar las dimensiones del array que entrará en el modelo, pasando a forma vector. Por otro, adimensionalizamos los valores de instensidad de los pixeles de las imágenes, para que estén entre 0 y 1 (antes estaban entre 0 y 255) y los convertimos en tipo float32.\n",
        "\n"
      ],
      "metadata": {
        "id": "lURH8Ng1ZRxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = train_images.reshape((60000, 28*28))\n",
        "train_images = train_images.astype('float32')/255\n",
        "\n",
        "test_images = test_images.reshape((10000, 28*28))\n",
        "test_images = test_images.astype('float32')/255\n",
        "\n"
      ],
      "metadata": {
        "id": "5YmYl6ogbdKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tambien va a ser necesario pasar las etiquetas a tipo \"categórico\"\n",
        "\n"
      ],
      "metadata": {
        "id": "XZn_cHZbX9fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "metadata": {
        "id": "2VB6W34fRpsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Entrenamiento del modelo\n",
        "\n",
        "Llegamos al paso culminante: el entrenamiento del modelo. Se hace utilizando el método fit(), con los parámetros imprescindibles del conjunto de datos de entrada, el número de iteraciones de cálculo, y el tamaño del lote de datos a utilizar en cada iteración.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8ABn93k5bpaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network.fit(train_images, train_labels, epochs = 5, batch_size = 128)"
      ],
      "metadata": {
        "id": "hwm9bmIPcNvo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4e54223-7203-45f4-c5aa-418f7ca8415b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 6s 11ms/step - loss: 0.2581 - accuracy: 0.9253\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1039 - accuracy: 0.9693\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0686 - accuracy: 0.9793\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0509 - accuracy: 0.9844\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.0375 - accuracy: 0.9885\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8a554365d0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Evaluación del modelo entrenado\n",
        "\n",
        "Ahora probamos el modelo entrenado utilizando el método evaluate() y los datos de test.\n",
        "\n"
      ],
      "metadata": {
        "id": "6hA7rKINCChO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss,test_acc = network.evaluate(test_images, test_labels)\n",
        "print(\"test_acc: \", test_acc)"
      ],
      "metadata": {
        "id": "S7ZI1jKxC7Ti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "040c26c4-44f9-44da-8d02-c2af95db0be9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0716 - accuracy: 0.9794\n",
            "test_acc:  0.9793999791145325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver que se obtiene una precisión del 97% con los nuevos datos de test."
      ],
      "metadata": {
        "id": "rFc5WnxlYRq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html /content/keras_MNIST.ipynb"
      ],
      "metadata": {
        "id": "kD_nRfMBYu9s",
        "outputId": "59e1f4af-da58-4194-930d-baf537f020f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook /content/keras_MNIST.ipynb to html\n",
            "[NbConvertApp] Writing 285516 bytes to /content/keras_MNIST.html\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ]
}